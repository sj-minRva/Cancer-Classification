{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "869d9ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['TSPAN6', 'TNMD', 'DPM1', 'SCYL3', 'FIRRM', 'FGR', 'CFH', 'FUCA2',\n",
      "       'GCLC', 'NFYA',\n",
      "       ...\n",
      "       'C8orf44-SGK3', 'SNORA74C-2', 'ELOA3BP', 'NPBWR1', 'ELOA3DP', 'LNCDAT',\n",
      "       'LOC124902537', 'RNF228', 'PANO1', 'classes'],\n",
      "      dtype='object', length=31575)\n",
      "Training set shape: (856, 31574)\n",
      "Testing set shape: (368, 31574)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\RHEA\\S7\\Project\\Code\\Cancer-Classification\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:19:49] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, roc_auc_score, cohen_kappa_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"C:/RHEA/S7/Project/Code/Datasets/BRCA_gene_expression.csv\", index_col=\"Unnamed: 0\")\n",
    "print(df.columns)\n",
    "\n",
    "# Encode the target variable\n",
    "le = LabelEncoder()\n",
    "df[\"classes\"] = le.fit_transform(df[\"classes\"])  # Convert categorical labels to 0 and 1\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(columns=[\"classes\"])\n",
    "y = df[\"classes\"]\n",
    "\n",
    "# Split dataset into training and testing sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Testing set shape:\", X_test.shape)\n",
    "\n",
    "df = df.iloc[:,1:]\n",
    "df.dropna()\n",
    "\n",
    "model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Get feature importances\n",
    "\n",
    "feature_importances = model.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Create a DataFrame for importances\n",
    "importance_df = pd.DataFrame({\n",
    "    'Gene': feature_names,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Select Top 1000 Genes\n",
    "# ---------------------------\n",
    "top_genes_df = importance_df.sort_values(by='Importance', ascending=False).head(1000)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c7b10e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9837\n",
      "Precision: 0.9375\n",
      "AUC: 0.9948\n",
      "Kappa: 0.9001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize and train the XGBoost classifier\n",
    "xgb_clf = xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric=\"logloss\")\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "y_pred_proba = xgb_clf.predict_proba(X_test)[:, 1]  # Probability estimates for AUC calculation\n",
    "\n",
    "# Evaluate model performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "print(f\"Kappa: {kappa:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "977d603d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\RHEA\\S7\\Project\\Code\\Cancer-Classification\\venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [09:21:16] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "C:\\Users\\rheam\\AppData\\Local\\Temp\\ipykernel_15136\\756624066.py:46: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, pd.DataFrame([[num_features, auc, accuracy, precision, kappa]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Features       AUC  Accuracy  Precision     Kappa\n",
      "0       10  0.997799  0.991848   0.942857  0.952025\n",
      "1       50  0.997446  0.989130   0.941176  0.935188\n",
      "2      100  0.997799  0.994565   0.944444  0.968428\n",
      "3      500  0.998151  0.991848   0.918919  0.953244\n",
      "4     1000  0.998415  0.991848   0.918919  0.953244\n"
     ]
    }
   ],
   "source": [
    "# Train XGBoost to get feature importances\n",
    "xgb_selector = XGBClassifier(n_estimators=500, learning_rate=0.05, max_depth=6,\n",
    "                             subsample=0.8, colsample_bytree=0.8, random_state=42,\n",
    "                             use_label_encoder=False, eval_metric=\"logloss\")\n",
    "\n",
    "xgb_selector.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importances = pd.Series(xgb_selector.feature_importances_, index=X.columns)\n",
    "\n",
    "# List of feature selection counts\n",
    "feature_counts = [10, 50, 100, 500, 1000]\n",
    "\n",
    "# Initialize a DataFrame to store results\n",
    "results_df = pd.DataFrame(columns=[\"Features\", \"AUC\", \"Accuracy\", \"Precision\", \"Kappa\"])\n",
    "\n",
    "# Loop through different feature counts\n",
    "for num_features in feature_counts:\n",
    "    # Select top N features\n",
    "    top_features = feature_importances.nlargest(num_features).index.tolist()\n",
    "\n",
    "    # Filter dataset with selected features\n",
    "    X_train_selected = X_train[top_features]\n",
    "    X_test_selected = X_test[top_features]\n",
    "\n",
    "    # Standardize the features (important for Logistic Regression)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_selected)\n",
    "    X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "    # Train Logistic Regression classifier\n",
    "    lr_clf = LogisticRegression(solver=\"liblinear\", C=1.0, random_state=42)\n",
    "    lr_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = lr_clf.predict(X_test_scaled)\n",
    "    y_pred_proba = lr_clf.predict_proba(X_test_scaled)[:, 1]  # Probability estimates for AUC calculation\n",
    "\n",
    "    # Evaluate model performance\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "    # Store results in DataFrame\n",
    "    results_df = pd.concat([results_df, pd.DataFrame([[num_features, auc, accuracy, precision, kappa]],\n",
    "                                                      columns=[\"Features\", \"AUC\", \"Accuracy\", \"Precision\", \"Kappa\"])],\n",
    "                                                      ignore_index=True)\n",
    "\n",
    "# Display final results\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "565b937c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save trained model components into a dict\n",
    "model_bundle = {\n",
    "    'scaler': scaler,            # If you used StandardScaler\n",
    "    'xgb_model': xgb_clf,        # Your trained XGBoost model\n",
    "    'meta_model': lr_clf,        # Logistic Regression meta-learner (if using stacking)\n",
    "    'metrics': {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'auc': auc,\n",
    "        'kappa': kappa\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save as .pkl\n",
    "joblib.dump(model_bundle, \"C:/RHEA/S7/Project/Code/Cancer-Classification/models/breast_cancer_xgb_lr.pkl\")\n",
    "print(\"Model saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
